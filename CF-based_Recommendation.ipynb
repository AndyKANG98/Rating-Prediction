{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #skip this line if you want to use gpu\n",
    "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "tensorflow.random.set_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE) is used to evaluate the performance of a recommendation algorithm, so we need to define the following utility function to compute the RMSE given the predicted ratings and the ground truth ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -pred: an array containing all predicted ratings\n",
    "    -actual: an array containing all ground truth ratings\n",
    "    \n",
    "return:\n",
    "    a scalar whose value is the rmse\n",
    "    \n",
    "'''\n",
    "def rmse(pred, actual):\n",
    "    # Ignore ratings with value zero.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Nueral Collaborative Filtering (NCF) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we implement two instantiations of NCF model. \n",
    "### The first instantiation computes the recommendation score (e.g., ratings) between a pair of user and item using dot product of their embeddings, which is equivalent to matrix factorization model for recommendation.\n",
    "### The second instantiation concatenates the user's and item's embeddings, then feed the the concatenated vector into a MLP to calculate the recommendation score. Adoption of MLP equips the model with high flexibility and non-linearity to effectively learn the interaction between user and item latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -n_users: number of user embedding vectors\n",
    "    -n_items: number of item embedding vectors\n",
    "    -embed_size: dimension of each embedding vector\n",
    "    -output_layer: which instantiation of NCF to use ('dot' or 'mlp')\n",
    "\n",
    "return:\n",
    "    a keras Model object for the constructed ncf model \n",
    "'''\n",
    "\n",
    "def build_ncf_model(n_users, n_items, embed_size, output_layer='dot'):\n",
    "    \n",
    "    # Get the users and items input\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    \n",
    "    # Get the embeddings of users and items\n",
    "    \n",
    "    user_emb = Embedding(output_dim=embed_size, input_dim=n_users, input_length=1)(user_input)\n",
    "    user_emb = Reshape((embed_size,))(user_emb)\n",
    "    item_emb = Embedding(output_dim=embed_size, input_dim=n_items, input_length=1)(item_input)\n",
    "    item_emb = Reshape((embed_size,))(item_emb)\n",
    "    \n",
    "\n",
    "    \n",
    "    if output_layer == 'dot':\n",
    "        # Compute the dot product of users' and items' embeddings as the model output\n",
    "        model_output = Dot(axes=1)([user_emb, item_emb])\n",
    "        \n",
    "    elif output_layer == 'mlp':\n",
    "        \n",
    "        # Concatenate the users' and items' embeddings as the input of MLP \n",
    "        mlp_input = Concatenate()([user_emb, item_emb])\n",
    "        \n",
    "        # First fully-connected layer\n",
    "        dense_1 = Dense(128, activation='relu')(mlp_input)\n",
    "        dense_1_dp = Dropout(0)(dense_1)\n",
    "        \n",
    "        # Second fully-connected layer\n",
    "        dense_2 = Dense(64, activation='relu')(dense_1_dp)\n",
    "        dense_2_dp = Dropout(0)(dense_2)\n",
    "        \n",
    "        # Final fully-connected layer to compute model output\n",
    "        model_output = Dense(1)(dense_2_dp)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input],\n",
    "                  outputs=model_output)\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and validation rating table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build two dictionaries mapping original user and item ids to corresponding indices in respective embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the set of all user ids and set of all business ids in train set\n",
    "user_set = set(tr_df.user_id.unique())\n",
    "business_set = set(tr_df.business_id.unique())\n",
    "\n",
    "#Build user vocabulary\n",
    "user_vocab = dict(zip(user_set, range(1, len(user_set) + 1)))\n",
    "\n",
    "#reserve the first row of the embedding matrix for users unseen in the training set\n",
    "user_vocab['unk'] = 0 \n",
    "n_users = len(user_vocab)\n",
    "\n",
    "#Build business vocabulary\n",
    "business_vocab = dict(zip(business_set, range(1, len(business_set) + 1)))\n",
    "#reserve the first row of the embedding matrix for businesses unseen in the training set\n",
    "business_vocab['unk'] = 0\n",
    "n_items = len(business_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the original user and item ids in train and valdiation set with indices in embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_users = tr_df.user_id.apply(lambda x: user_vocab[x]).values\n",
    "tr_items = tr_df.business_id.apply(lambda x: business_vocab[x]).values\n",
    "val_users = val_df.user_id.apply(lambda x: user_vocab[x] if x in user_vocab else 0).values\n",
    "val_items = val_df.business_id.apply(lambda x: business_vocab[x] if x in business_vocab else 0).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ratings in train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ratings = tr_df.stars.values\n",
    "val_ratings = val_df.stars.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the NCF model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_ncf_model(\n",
    "        n_users, n_items, \n",
    "        embed_size=100,\n",
    "        output_layer='mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using Adam optimizer and mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kang\\Anaconda3\\envs\\comp4332\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60080/60080 [==============================] - 30s 508us/step - loss: 1.1926\n"
     ]
    }
   ],
   "source": [
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "tensorflow.random.set_seed(2020)\n",
    "\n",
    "model.compile(optimizer='adagrad', loss='mse')\n",
    "\n",
    "history = model.fit(\n",
    "        [tr_users, tr_items], \n",
    "        tr_ratings, \n",
    "        epochs=1, \n",
    "        verbose=1,\n",
    "        callbacks=[ModelCheckpoint('model.h5')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on train and validation sets using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  0.9930638128415068\n",
      "VALID RMSE:  1.0836642741354767\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([tr_users, tr_items])\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = model.predict([val_users, val_items])\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  0.902204873281488\n",
      "VALID RMSE:  1.0655312708196742\n"
     ]
    }
   ],
   "source": [
    "# embedding = 100\n",
    "# dropout = 1.0\n",
    "# optimizer = adagrad\n",
    "y_pred = model.predict([tr_users, tr_items])\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = model.predict([val_users, val_items])\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  0.9527370087846538\n",
      "VALID RMSE:  1.0672388630674958\n"
     ]
    }
   ],
   "source": [
    "# embedding = 100\n",
    "# dropout = 1.0\n",
    "# optimizer = adagrad\n",
    "y_pred = model.predict([tr_users, tr_items])\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = model.predict([val_users, val_items])\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
